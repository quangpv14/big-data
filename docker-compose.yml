# docker-compose.yml
version: "3"
services:
  spark-submit-app:
    build:
      context: spark
      dockerfile: Dockerfile
    environment:
      - SPARK_APPLICATION_PYTHON_LOCATION=/app/app.py
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
      - kafka
      - crawler
  
  # HDFS
  namenode:
    image: bde2020/hadoop-namenode:1.1.0-hadoop2.8-java8
    container_name: namenode
    volumes:
      - ./data/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    ports:
      - 51071:50070   # Web UI quản lý Namenode
      - 8020:8020     # RPC client kết nối (Spark, Hive...)
  datanode:
    image: bde2020/hadoop-datanode:1.1.0-hadoop2.8-java8
    depends_on:
      - namenode
    volumes:
      - ./data/datanode:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    ports:
      - 51075:50075 # Web UI của Datanode

  # Hive + Metastore
  hive-server:
    image: bde2020/hive:2.1.0-postgresql-metastore
    container_name: hive-server
    env_file:
      - ./hadoop.env
    environment:
      - "HIVE_CORE_CONF_javax_jdo_option_ConnectionURL=jdbc:postgresql://hive-metastore/metastore"
    ports:
      - "10000:10000"

  hive-metastore:
    image: bde2020/hive:2.1.0-postgresql-metastore
    container_name: hive-metastore
    env_file:
      - ./hadoop.env
    command: /opt/hive/bin/hive --service metastore

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.1.0

  # Spark
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    ports:
      - "8079:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
  spark-worker-1:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
  spark-worker-2:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"

  # Hue - HDFS File Browser
  hue:
    #image: bde2020/hdfs-filebrowser:3.11
    image: gethue/hue:latest
    volumes:
      - ./hue/conf/hue.ini:/usr/share/hue/desktop/conf/hue.ini
    environment:
      - HUE_CONF_DIR=/usr/share/hue/desktop/conf
      - HUE_DESKTOP_CONF_DIR=/usr/share/hue/desktop/conf
      - NAMENODE_HOST=namenode
    depends_on:
      - namenode
    ports:
      - 8888:8888
    
  # Kafka & Zookeeper
  zookeeper:
    restart: always
    image: bitnami/zookeeper:latest
    ports:
      - "2181:2181"
    volumes:
      - "zookeeper-volume:/bitnami"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
  kafka:
    restart: always
    image: bitnami/kafka:3.4.1
    ports:
      - "9093:9093"
    volumes:
      - "kafka-volume:/bitnami"
    environment:
      #- KAFKA_CFG_NODE_ID=0
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_ENABLE_KRAFT=false
      #- KAFKA_CFG_PROCESS_ROLES=broker
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL://localhost:9093
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
    depends_on:
      - zookeeper
  crawler:
    build:
      context: crawler
      dockerfile: Dockerfile
    ports:
      - "5000:5000"
    depends_on:
      - kafka
      - zookeeper
volumes:
  kafka-volume:
  zookeeper-volume:

